#if merging first and then extract features
#getting joint information
from pathlib import Path
import numpy as np
import argparse
import cv2
import os.path as osp
import h5py
import tarfile
import re
import os
from hamer.utils.renderer import Renderer, cam_crop_to_full
from library.readh5 import ReadH5
from PIL import Image
import torch
import pandas as pd
import matplotlib.pyplot as plt
import math
import io
from hamer.models import MANO
from hamer.configs import CACHE_DIR_HAMER
from hamer.models import HAMER, download_models, load_hamer, DEFAULT_CHECKPOINT
from hamer.utils import recursive_to
from hamer.datasets.vitdet_dataset import ViTDetDataset, DEFAULT_MEAN, DEFAULT_STD
from hamer.utils.renderer import Renderer, cam_crop_to_full
from vitpose_model import ViTPoseModel
import torch.nn as nn

LIGHT_BLUE=(0.65098039,  0.74117647,  0.85882353)
LIGHT_BLUE = (LIGHT_BLUE[2], LIGHT_BLUE[1], LIGHT_BLUE[0])


def project_hand_pose(hand_pose, camera_matrix, pred_cam_right):
    # Project 3D hand pose to 2D using the camera matrix
    default_camera_matrix = np.array([[1, 0, 0, pred_cam_right[0]], [0, 1, 0,pred_cam_right[1]], [0, 0, 1,pred_cam_right[2]]])
    hand_pose_homogeneous = np.hstack((hand_pose, np.ones((hand_pose.shape[0], 1))))
    # print("hand_pose_homogeneous",np.shape(hand_pose_homogeneous))
    # print("camera_matrix",np.shape(camera_matrix))
    projected_pose = camera_matrix @ default_camera_matrix@ hand_pose_homogeneous.T
    projected_pose /= projected_pose[2, :]
    return projected_pose[:2, :].T

def calculate_fingertip_centre(fingertips):
    fingertips_array = np.array(fingertips)
    if np.all(fingertips_array ==0):
        return None
    centre = np.mean(fingertips_array, axis=0)
    return centre

def distance(a, b):
    if len(a) == 2 and len(b) == 2:
        return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)
    elif len(a) == 3 and len(b) == 3:
        return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2 + (a[2] - b[2]) ** 2)


def main():

    parser = argparse.ArgumentParser(description='Sliding Window IoU Detection')
    parser.add_argument('--hfile', type=str, help='Path to the directory containing left.npy and right.npy files')
    args = parser.parse_args()
    
    mano_cfg = {'data_dir': '_DATA/data/', 'model_path':'_DATA/data/mano/',
                'gender':'neutral','num_hand_joints':15,'mean_params':'./_DATA/data/mano_mean_params.npz','create_body_pose': False}
    mano = MANO(**mano_cfg)

    read_h5file = ReadH5(f'{args.hfile}.h5')
    f = h5py.File(f'{args.hfile}.h5')
    dataset = f['video/hand_pose']
    total_frames = dataset.shape[0]
    cnt = 0

    hf = h5py.File(f'{args.hfile}f86.h5', 'w')
    g1 = hf.create_group('video')
    g1.create_dataset('features', (total_frames, 86))
    data1 = hf['video/features']
    # g1.create_dataset('label', (total_frames, 1))
    # data2 = hf['video/label']
    g1.create_dataset('cleanedlabel', (total_frames, 1))
    data9 = hf['video/cleanedlabel']
    g1.create_dataset('video_name', (total_frames, 1), dtype='i8') 
    data4 = hf['video/video_name']
    g1.create_dataset('frame_number', (total_frames, 1))
    data7 = hf['video/frame_number']
    str_dtype = h5py.string_dtype(encoding='utf-8')
    g1.create_dataset('subtitle', (total_frames,), dtype=str_dtype)
    data8 = hf['video/subtitle']
    predictions = np.zeros(total_frames)


    for cnt in range(total_frames):
        all_verts = []
        all_cam_t = []
        all_right = []   

        data = read_h5file.read_sequence_slice('video/hand_pose', cnt)
        #data3 = read_h5file.read_sequence_slice('video/label', cnt)
        data5 = read_h5file.read_sequence_slice('video/video_name', cnt)
        data8[cnt]= read_h5file.read_sequence_slice('video/subtitle', cnt)

        #data2[cnt] = data3
        data4[cnt] = data5
        data7[cnt] = int(data[0])
        # frame_number = int(data[0])
        # cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
        # ret, frame = cap.read()


        left_exist = int(data[1])
        right_exist = int(data[162])
        img_size = torch.tensor([[456., 256.]])
        FOCAL_LENGTH = 5000
        MODEL_IMG_SIZE = 224
        scaled_focal_length = FOCAL_LENGTH / MODEL_IMG_SIZE * img_size.max()

        
        camera_matrix = np.array([[scaled_focal_length , 0, img_size[0][0]/2],
                        [0, scaled_focal_length , img_size[0][1]/2],
                        [0, 0, 1]])
        if left_exist:
            arr_left = data[2:162]
            box_center_left = torch.tensor([arr_left[:2]])
            box_size_left = torch.tensor([arr_left[2]])
            pred_cam_left = torch.tensor([arr_left[3:6]])
            global_orient_left = torch.tensor([np.reshape(arr_left[6:15], (1, 3, 3))])
            hand_pose_left = torch.tensor([np.reshape(arr_left[15:150], (15, 3, 3))])
            betas_left = torch.tensor([arr_left[150:]])
            is_right=0
            mano_output_left = mano(**{'global_orient': global_orient_left, 'hand_pose': hand_pose_left, 'betas': betas_left}, pose2rot=False)
            pred_keypoints_3d_left = mano_output_left.joints.reshape(-1, 3).numpy()
            pred_keypoints_3d_left[:,0] = (2*is_right-1)*pred_keypoints_3d_left[:,0]
            pred_cam_t_full_left = cam_crop_to_full(pred_cam_left, box_center_left, box_size_left, img_size, scaled_focal_length).detach().cpu().numpy()
            projected_pose_left = project_hand_pose(pred_keypoints_3d_left, camera_matrix, pred_cam_t_full_left[0])
            feature_left = projected_pose_left.flatten()
            fingertips_left = [projected_pose_left[4], projected_pose_left[8],
                                       projected_pose_left[12], projected_pose_left[16], projected_pose_left[20]]
            left_centre = calculate_fingertip_centre(fingertips_left)
            fingertips_left_3d = [pred_keypoints_3d_left[4], pred_keypoints_3d_left[8],
                                       pred_keypoints_3d_left[12], pred_keypoints_3d_left[16], pred_keypoints_3d_left[20]]
            left_centre_3d = calculate_fingertip_centre(fingertips_left_3d)


        if right_exist:
            arr_right = data[163:]
            box_center_right = torch.tensor([arr_right[:2]])
            box_size_right = torch.tensor([arr_right[2]])
            pred_cam_right = torch.tensor([arr_right[3:6]])
            global_orient_right = torch.tensor([np.reshape(arr_right[6:15], (1, 3, 3))])
            hand_pose_right = torch.tensor([np.reshape(arr_right[15:150], (15, 3, 3))])
            betas_right = torch.tensor([arr_right[150:]])

            mano_output_right = mano(**{'global_orient': global_orient_right, 'hand_pose': hand_pose_right, 'betas': betas_right}, pose2rot=False)
            pred_keypoints_3d_right = mano_output_right.joints.reshape(-1, 3).numpy()
            pred_cam_t_full_right = cam_crop_to_full(pred_cam_right, box_center_right, box_size_right, img_size, scaled_focal_length).detach().cpu().numpy()
            projected_pose_right = project_hand_pose(pred_keypoints_3d_right, camera_matrix, pred_cam_t_full_right[0])
            feature_right = projected_pose_right.flatten() 
            fingertips_right = [projected_pose_right[4], projected_pose_right[8],
                                        projected_pose_right[12], projected_pose_right[16], projected_pose_right[20]]
            right_centre = calculate_fingertip_centre(fingertips_right)
            fingertips_right_3d = [pred_keypoints_3d_right[4], pred_keypoints_3d_right[8],
                                        pred_keypoints_3d_right[12], pred_keypoints_3d_right[16], pred_keypoints_3d_right[20]]
            right_centre_3d = calculate_fingertip_centre(fingertips_right_3d)
        if right_exist and left_exist:
            features = np.concatenate([feature_right, feature_left,  np.array([distance(right_centre,left_centre)]),np.array([distance(right_centre_3d, left_centre_3d)])])
        else:
            features = np.zeros(feature_right.shape[0]+ feature_left.shape[0]+2)
        data1[cnt] = features
        features=np.array(features)
        features = features.reshape(1, -1)
        features = scaler.transform(features)
        features=torch.FloatTensor(features)

        cnt+=1



if __name__ == '__main__':
    main()
